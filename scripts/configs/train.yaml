checkpoint_path: "tmp" # Path to save the checkpoints
n_pcs: 20 # Number of principal components to use

# Configuration for the model
model:
  epsilon: 0.05 # The entropic regularization for the outer loss
  debias: true # Whether to use debiasing
  teacher_forcing: true # Whether to use teacher forcing
  quadratic: true # If true, use FGW instead of Sinkhorn
  fused: 5.0 # Fused penalty, used if quadratic=true
  seed: ??? # The random seed

# Configuration for the neural potential function
potential:
  features: [128, 128]

# Configuration for the proximal step. Type may be:
# "explicit", "monge_implicit", "icnn_implicit"
step:
  type: "explicit" # The type of proximal step
  implicit_diff: true # Used if type="*_implicit"
  maxiter: 100 # Used if type="*_implicit"

# Configuration for the optimizer
optimizer:
  train_val_split: 0.75 # Proportion of train in the train/val split
  learning_rate: 0.01 # Learning rate of the optimizer
  checkpoint_interval: 1 # Try to save weights every x iter.
  max_iter: 15_000 # Maximum number of iter
  batch_size: 1_000 # The batch size
  min_delta: 0.0 # The minimum delta for early stopping
  patience: 150

# Configuration for Weights & Biases
wandb:
  mode: "offline"
